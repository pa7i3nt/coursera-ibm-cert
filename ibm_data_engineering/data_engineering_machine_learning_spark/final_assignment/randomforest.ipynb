{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f70b2f8-6f47-4399-a575-1e610034321a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "Starting installation...\n",
      "Please <<RESTART YOUR KERNEL>> (Kernel->Restart Kernel and Clear All Outputs)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export version=`python --version |awk '{print $2}' |awk -F\".\" '{print $1$2}'`\n",
    "\n",
    "echo $version\n",
    "\n",
    "if [ $version == '36' ] || [ $version == '37' ]; then\n",
    "    echo 'Starting installation...'\n",
    "    pip3 install pyspark==2.4.8 wget==3.2 pyspark2pmml==0.5.1 > install.log 2> install.log\n",
    "    if [ $? == 0 ]; then\n",
    "        echo 'Please <<RESTART YOUR KERNEL>> (Kernel->Restart Kernel and Clear All Outputs)'\n",
    "    else\n",
    "        echo 'Installation failed, please check log:'\n",
    "        cat install.log\n",
    "    fi\n",
    "elif [ $version == '38' ] || [ $version == '39' ]; then\n",
    "    pip3 install pyspark==3.1.2 wget==3.2 pyspark2pmml==0.5.1 > install.log 2> install.log\n",
    "    if [ $? == 0 ]; then\n",
    "        echo 'Please <<RESTART YOUR KERNEL>> (Kernel->Restart Kernel and Clear All Outputs)'\n",
    "    else\n",
    "        echo 'Installation failed, please check log:'\n",
    "        cat install.log\n",
    "    fi\n",
    "else\n",
    "    echo 'Currently only python 3.6, 3.7 , 3.8 and 3.9 are supported, in case you need a different version please open an issue at https://github.com/IBM/claimed/issues'\n",
    "    exit -1\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "009a2cdc-cc3c-4808-9139-77859f62c651",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @param data_dir temporal data storage for local execution\n",
    "# @param data_csv csv path and file name (default: data.csv)\n",
    "# @param data_parquet path and parquet file name (default: data.parquet)\n",
    "# @param master url of master (default: local mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922123f9-590d-4002-ba15-2e0291011493",
   "metadata": {},
   "source": [
    "# import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee375060-b67c-43ad-98c7-a1d716fd5fd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark2pmml import PMMLBuilder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "import logging\n",
    "import shutil\n",
    "import site\n",
    "import sys\n",
    "import wget\n",
    "import re\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe86831-0dfc-4e1b-9111-32c4ea425509",
   "metadata": {},
   "source": [
    "# load data.parquet and convert to csv as csv_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb82cb6a-3149-4473-879a-5b745c68442f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_csv = os.environ.get('data_csv', 'data.csv')\n",
    "# data_parquet = os.environ.get('data_parquet', 'data.parquet')\n",
    "master = os.environ.get('master', \"local[*]\")\n",
    "data_dir = os.environ.get('data_dir', './component-library/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6620ff49-a452-40a2-9181-3daa8b5697d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./component-library/data/data.parquet\n"
     ]
    }
   ],
   "source": [
    "data_parquet = 'data.parquet'\n",
    "data_csv = 'data.csv'\n",
    "\n",
    "print(data_dir + data_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9240a90a-a78f-49c4-818a-302011393d94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "skip = False\n",
    "if os.path.exists(data_dir + data_csv):\n",
    "    skip = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d908708-00d1-4b8c-9e34-92fd718471fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip:\n",
    "    sc = SparkContext.getOrCreate(SparkConf().setMaster(master))\n",
    "    spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f69c3f08-eb32-45c9-bfb8-304c3d891813",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip:\n",
    "    df = spark.read.parquet(data_dir + data_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "996e0c3d-b6fa-481f-b7c3-75aa9cf0a719",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip:\n",
    "    if os.path.exists(data_dir + data_csv):\n",
    "        shutil.rmtree(data_dir + data_csv)\n",
    "    df.coalesce(1).write.option(\"header\", \"true\").csv(data_dir + data_csv)\n",
    "    file = glob.glob(data_dir + data_csv + '/part-*')\n",
    "    shutil.move(file[0], data_dir + data_csv + '.tmp')\n",
    "    shutil.rmtree(data_dir + data_csv)\n",
    "    shutil.move(data_dir + data_csv + '.tmp', data_dir + data_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9c9c77-ea01-48eb-be3a-fd6b176d7479",
   "metadata": {},
   "source": [
    "# ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b7ba80a-1e79-49d3-8213-a990bcecde16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#check python and spark compatible version\n",
    "if sys.version[0:3] == '3.9':\n",
    "    url = ('https://github.com/jpmml/jpmml-sparkml/releases/download/1.7.2/'\n",
    "           'jpmml-sparkml-executable-1.7.2.jar')\n",
    "    wget.download(url)\n",
    "    shutil.copy('jpmml-sparkml-executable-1.7.2.jar',\n",
    "                site.getsitepackages()[0] + '/pyspark/jars/')\n",
    "elif sys.version[0:3] == '3.8':\n",
    "    url = ('https://github.com/jpmml/jpmml-sparkml/releases/download/1.7.2/'\n",
    "           'jpmml-sparkml-executable-1.7.2.jar')\n",
    "    wget.download(url)\n",
    "    shutil.copy('jpmml-sparkml-executable-1.7.2.jar',\n",
    "                site.getsitepackages()[0] + '/pyspark/jars/')\n",
    "elif sys.version[0:3] == '3.7':\n",
    "    url = ('https://github.com/jpmml/jpmml-sparkml/releases/download/1.5.12/'\n",
    "           'jpmml-sparkml-executable-1.5.12.jar')\n",
    "    wget.download(url)\n",
    "elif sys.version[0:3] == '3.6':\n",
    "    url = ('https://github.com/jpmml/jpmml-sparkml/releases/download/1.5.12/'\n",
    "           'jpmml-sparkml-executable-1.5.12.jar')\n",
    "    wget.download(url)\n",
    "else:\n",
    "    raise Exception('Currently only python 3.6 , 3.7, 3,8 and 3.9 is supported, in case '\n",
    "                    'you need a different version please open an issue at '\n",
    "                    'https://github.com/IBM/claimed/issues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2910cde1-6289-40f1-93e1-e272ff4e1b26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_parquet = os.environ.get('data_parquet',\n",
    "                              'data.parquet')  # input file name (parquet)\n",
    "master = os.environ.get('master',\n",
    "                        \"local[*]\")  # URL to Spark master\n",
    "model_target = os.environ.get('model_target',\n",
    "                              \"model.xml\")  # model output file name\n",
    "data_dir = os.environ.get('data_dir',\n",
    "                          './component-library/data/')  # temporary directory for data\n",
    "input_columns = os.environ.get('input_columns',\n",
    "                               '[\"x\", \"y\", \"z\"]')  # input columns to consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c0df8bb-d3c7-4077-94fc-bb4c7d87de6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = list(\n",
    "    map(lambda s: re.sub('$', '\"', s),\n",
    "        map(\n",
    "            lambda s: s.replace('=', '=\"'),\n",
    "            filter(\n",
    "                lambda s: s.find('=') > -1 and bool(re.match(r'[A-Za-z0-9_]*=[.\\/A-Za-z0-9]*', s)),\n",
    "                sys.argv\n",
    "            )\n",
    "    )))\n",
    "\n",
    "for parameter in parameters:\n",
    "    logging.warning('Parameter: ' + parameter)\n",
    "    exec(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0574b5f-5bfe-496f-820e-b1585e0b1d9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/18 02:47:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/18 02:48:01 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "#setup spark session\n",
    "conf = SparkConf().setMaster(master)\n",
    "#if sys.version[0:3] == '3.6' or sys.version[0:3] == '3.7':\n",
    "conf.set(\"spark.jars\", 'jpmml-sparkml-executable-1.5.12.jar')\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = sqlContext.sparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87ac17f2-cc1c-4531-a9bf-b08533cd39e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyterlab-giangfnptit:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f56f0be0b90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f535cad-33a5-4fd5-8267-c32bee00a61a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:====================================================>    (25 + 2) / 27]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: integer (nullable = true)\n",
      " |-- y: integer (nullable = true)\n",
      " |-- z: integer (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Read csv data\n",
    "df = spark.read.csv(data_dir + data_csv, header = True, inferSchema = True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e3d3f77-5058-431c-ad41-243702bbd59c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# register a corresponding query table\n",
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "257720d3-ca20-4a25-b43c-1b63e3f95d5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cast x, y, z columns from int to double\n",
    "from pyspark.sql.types import DoubleType\n",
    "df = df.withColumn(\"x\", df.x.cast(DoubleType()))\n",
    "df = df.withColumn(\"y\", df.y.cast(DoubleType()))\n",
    "df = df.withColumn(\"z\", df.z.cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c3e5395-1695-4e41-b1b7-e9d1781ce803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# random split with seed = 1\n",
    "splits = df.randomSplit([0.8, 0.2], seed  = 1)\n",
    "df_train = splits[0]\n",
    "df_test = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49e5575a-d7be-40f5-89fd-1e50cb7ce8e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=eval(input_columns),\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "normalizer = MinMaxScaler(inputCol=\"features\", outputCol=\"features_norm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96a637b-017c-4c30-a67b-785a62d99e0f",
   "metadata": {},
   "source": [
    "## Iterate through a list of trees and depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c20e7c-2aa2-4915-91b5-1161c86c7dde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for hyperparameters: numTrees = 10, maxDepth = 5.000000e+00, prediction = = 0.440845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for hyperparameters: numTrees = 10, maxDepth = 5.000000e+00, prediction = = 0.444832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for hyperparameters: numTrees = 10, maxDepth = 7.000000e+00, prediction = = 0.464872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for hyperparameters: numTrees = 10, maxDepth = 7.000000e+00, prediction = = 0.462512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for hyperparameters: numTrees = 20, maxDepth = 5.000000e+00, prediction = = 0.441976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for hyperparameters: numTrees = 20, maxDepth = 5.000000e+00, prediction = = 0.445121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for hyperparameters: numTrees = 20, maxDepth = 7.000000e+00, prediction = = 0.467804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 182:================>                                       (8 + 8) / 27]"
     ]
    }
   ],
   "source": [
    "numTrees = [10, 20]\n",
    "maxDepth = [5, 7]\n",
    "seed_list = [1, None]\n",
    "\n",
    "acc_list = []\n",
    "tuning_list = []\n",
    "\n",
    "for n in numTrees:\n",
    "    for m in maxDepth:\n",
    "        for seed in seed_list:    \n",
    "            tuning_list.append([n, m, seed])\n",
    "            rf = RandomForestClassifier(numTrees = n, maxDepth=m, seed=seed)\n",
    "            pipeline = Pipeline(stages=[indexer, vectorAssembler, normalizer, rf])\n",
    "            #train\n",
    "            model = pipeline.fit(df_train)\n",
    "            #test\n",
    "            prediction = model.transform(df_train)\n",
    "            binEval = MulticlassClassificationEvaluator(). \\\n",
    "                setMetricName(\"accuracy\"). \\\n",
    "                setPredictionCol(\"prediction\"). \\\n",
    "                setLabelCol(\"label\")\n",
    "\n",
    "            acc = binEval.evaluate(prediction)\n",
    "            acc_list.append(acc)\n",
    "            #print accuracy\n",
    "            print(\"accuracy for hyperparameters: numTrees = %d, maxDepth = %e, prediction = = %g\"\n",
    "                %(n, m, acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777bec7d-b1b4-4959-be37-ad86aa5b2243",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Report the combination of hyperparameters that yielded the highest accuracy\n",
    "max_index = np.argmax(acc_list)\n",
    "print('Max accuracy:', acc_list[max_index], 'numTrees:', tuning_list[max_index][0],'maxDepth:', tuning_list[max_index][1], 'seed:', tuning_list[max_index][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b86e96-d5f0-4325-b997-8054b9649286",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pmmlBuilder = PMMLBuilder(sc, df_train, model)\n",
    "pmmlBuilder.buildFile(data_dir + \"random_forest.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121b2630-62af-4f89-b4ad-bf66198c1934",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
